{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pretrained_Word_Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih8Dg0DinC4q",
        "outputId": "f8afd5ea-a273-4b9d-962b-6fa36a2a44bb"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIQNBgqGn6Kb"
      },
      "source": [
        "**Descomprimimos la base de datos IMDB original**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmlQSzskrGIS"
      },
      "source": [
        "Enlace para acceder a dicha base de datos: http://ai.stanford.edu/~amaas/data/sentiment/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqFgq-3EmPUg",
        "outputId": "d7852226-9d1f-4726-f77d-f552a9243fb7"
      },
      "source": [
        "import tarfile\r\n",
        "tar = tarfile.open('/content/drive/My Drive/Natural_Language_Processing/aclImdb_v1.tar.gz')\r\n",
        "tar.extractall('/content/')\r\n",
        "tar.close"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method TarFile.close of <tarfile.TarFile object at 0x7efd88bf6860>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh0y8f--oZko"
      },
      "source": [
        "**Almacenamos las críticas del conjunto de entrenamiento y sus etiquetas asociadas en listas apropiadas.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvpj0rewoYRB"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "imdb_dir = '/content/aclImdb'\r\n",
        "train_dir = os.path.join(imdb_dir, 'train')\r\n",
        "\r\n",
        "labels = []\r\n",
        "texts = []\r\n",
        "\r\n",
        "for label_type in ['neg', 'pos']:\r\n",
        "    dir_name = os.path.join(train_dir, label_type)\r\n",
        "    for fname in os.listdir(dir_name):\r\n",
        "        if fname[-4:] == '.txt':\r\n",
        "            f = open(os.path.join(dir_name, fname))\r\n",
        "            texts.append(f.read())\r\n",
        "            f.close()\r\n",
        "            if label_type == 'neg':\r\n",
        "                labels.append(0)\r\n",
        "            else:\r\n",
        "                labels.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLUoUvhOo__K"
      },
      "source": [
        "**Tokenizamos las críticas del conjunto de entrenamiento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeKoKyDQpFHg"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "maxlen = 100  # Usaremos sólo las 100 primeras palabras de cada críticas\r\n",
        "training_samples = 200  # El entrenamiento se realizará con 200 muestras\r\n",
        "validation_samples = 10000  # El conjunto de validación estará compuesto de 10000 muestras\r\n",
        "max_words = 10000  # Consideraremos únicamente las 10,000 palabras iniciales de la base de datos\r\n",
        "tokenizer = Tokenizer(num_words=max_words)\r\n",
        "tokenizer.fit_on_texts(texts)\r\n",
        "sequences = tokenizer.texts_to_sequences(texts)\r\n",
        "\r\n",
        "word_index = tokenizer.word_index\r\n",
        "print('Se encontraron %s tokens únicos.' % len(word_index))\r\n",
        "\r\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\r\n",
        "\r\n",
        "labels = np.asarray(labels)\r\n",
        "print('Dimensiones del tensor de datos:', data.shape)\r\n",
        "print('Dimensiones del tensor de etiquetas:', labels.shape)\r\n",
        "\r\n",
        "# Partición de los datos en conjunto de entrenaminto y conjunto de validación.\r\n",
        "# Pero primero, apliquemos shuffling en los datos, ya que estos están ordenados\r\n",
        "# (todas las críticas positivas primeros y luego todas las críticas negativas).\r\n",
        "indices = np.arange(data.shape[0])\r\n",
        "np.random.shuffle(indices)\r\n",
        "data = data[indices]\r\n",
        "labels = labels[indices]\r\n",
        "\r\n",
        "x_train = data[:training_samples]\r\n",
        "y_train = labels[:training_samples]\r\n",
        "x_val = data[training_samples: training_samples + validation_samples]\r\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uLEu-laqGBr"
      },
      "source": [
        "# **Base de datos GloVe**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnGLBcbGqODB"
      },
      "source": [
        "La base de datos GloVe (Global Vectors for Word Representation) fue desarrollada en 2014 por investigadores de la Universidad de Stanford.<br>\r\n",
        "Esta base de datos se ha actualizado continuamente, aunque en nuestro caso, emplearemos la versión del 2014, la cual se compone de word embeddings de Wikipedia 2014. Los word embeddings de esta base de datos representan 400,000 palabras embebidas en vectores de 100 dimensiones.<br>\r\n",
        "Enlace para acceder a la base de datos: https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_rxZGsXrSNA"
      },
      "source": [
        "**Generamos un índice que mapea palabras a su representación vectorial**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXCq2_oPrXto"
      },
      "source": [
        "glove_dir = '/home/ubuntu/data/'\r\n",
        "\r\n",
        "embeddings_index = {}\r\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\r\n",
        "for line in f:\r\n",
        "    values = line.split()\r\n",
        "    word = values[0]\r\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\r\n",
        "    embeddings_index[word] = coefs\r\n",
        "f.close()\r\n",
        "\r\n",
        "print('Se encontraron %s vectores de palabras.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VR2vM5Mrktv"
      },
      "source": [
        "Ahora, construyamos una matriz embebida que pueda alimentar a la capa Embedding. Esta matriz debe tener dimensiones (max_words_ embedding_dim), donde cada entrada i contiene la representación vector embedding de la palabra con índice i de nuestro índice de palabras diseñada previamente durante la tokenización. Es importante señalar que el índice 0 no tiene asociado ninguna palabra o token, pues es un placeholder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDEHX27WsbY6"
      },
      "source": [
        "embedding_dim = 100\r\n",
        "\r\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\r\n",
        "for word, i in word_index.items():\r\n",
        "    embedding_vector = embeddings_index.get(word)\r\n",
        "    if i < max_words:\r\n",
        "        if embedding_vector is not None:\r\n",
        "            # Las palabras que no se encuentren en el índice embeddings serán representadas por vectores de ceros\r\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU0N74EhsufO"
      },
      "source": [
        "# **Definimos la arquitectura**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwwZWwouswhY"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(32, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbRTSYets1UX"
      },
      "source": [
        "# **Cargamos la base de datos GloVe a nuestro modelo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7nHbTs6s5K3"
      },
      "source": [
        "model.layers[0].set_weights([embedding_matrix])\r\n",
        "model.layers[0].trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvl8wRKEs98I"
      },
      "source": [
        "# **Entrenamiento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUKN2zhUs_Uv"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\r\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\r\n",
        "\r\n",
        "es = EarlyStopping(monitor= 'val_acc', mode='max', patience = 7, verbose=1)\r\n",
        "mc = ModelCheckpoint('/content/model_pretrained_embedding_imdb.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\r\n",
        "\r\n",
        "model.compile(optimizer='rmsprop',\r\n",
        "              loss='binary_crossentropy',\r\n",
        "              metrics=['acc'])\r\n",
        "history = model.fit(x_train, y_train,\r\n",
        "                    epochs=10,\r\n",
        "                    batch_size=32,\r\n",
        "                    validation_data=(x_val, y_val),\r\n",
        "                    callbacks = [es, mc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4AIfeaS8xcN"
      },
      "source": [
        "# **Curvas de precisión y pérdida**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOyTdAbv8xcP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "ec9efe08-8959-4d10-c4a2-d3be1328a56c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "max_val_acc = max(val_acc)\n",
        "max_val_acc_epoch= val_acc.index(max(val_acc)) + 1\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "min_val_loss = min(val_loss)\n",
        "min_val_loss_epoch= val_loss.index(min(val_loss)) + 1\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'go', label='Precisión en el entrenamiento')\n",
        "plt.plot(epochs, val_acc, 'r', label='Precisión en la validación')\n",
        "plt.plot(max_val_acc_epoch, max_val_acc, 'bo', label='Maximum accuracy')\n",
        "plt.title('Precisión durante el entrenamiento y la validación')\n",
        "plt.legend()\n",
        "print(\"Best accuracy epoch : % d, Value : % .1f\" %(max_val_acc_epoch, max_val_acc)) \n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'go', label='Pérdida en el entrenamiento')\n",
        "plt.plot(epochs, val_loss, 'r', label='Pérdida en la validación')\n",
        "plt.plot(min_val_loss_epoch, min_val_loss, 'bo', label='Minimum loss')\n",
        "plt.title('Pérdida durante el entrenamiento y la validación')\n",
        "plt.legend()\n",
        "print(\"Best loss epoch : % d, Value : % .10f\" %(min_val_loss_epoch, min_val_loss))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-58243a6e143d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmax_val_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmBwcAF4tpfp"
      },
      "source": [
        "# **Evaluamos el modelo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvZlMG9RtrUw"
      },
      "source": [
        "test_dir = os.path.join(imdb_dir, 'test')\r\n",
        "\r\n",
        "labels = []\r\n",
        "texts = []\r\n",
        "\r\n",
        "for label_type in ['neg', 'pos']:\r\n",
        "    dir_name = os.path.join(test_dir, label_type)\r\n",
        "    for fname in sorted(os.listdir(dir_name)):\r\n",
        "        if fname[-4:] == '.txt':\r\n",
        "            f = open(os.path.join(dir_name, fname))\r\n",
        "            texts.append(f.read())\r\n",
        "            f.close()\r\n",
        "            if label_type == 'neg':\r\n",
        "                labels.append(0)\r\n",
        "            else:\r\n",
        "                labels.append(1)\r\n",
        "\r\n",
        "sequences = tokenizer.texts_to_sequences(texts)\r\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\r\n",
        "y_test = np.asarray(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-XTlm-0tt3H"
      },
      "source": [
        "model.load_weights('model_pretrained_embedding_imdb.h5')\r\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}