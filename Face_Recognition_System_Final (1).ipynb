{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face_Recognition_System_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHyF7lBP7RlV"
      },
      "source": [
        "Terravic original: http://vcipl-okstate.org/pbvs/bench/Data/04/download.html <br>\n",
        "Link Google Drive: https://drive.google.com/drive/folders/1HhNXDqfuXckBl8EOBi_NEJcyNIVKIcdp?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wFlHdnVW23Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b093a8-05aa-49c8-b229-914bb1a12341"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P71-MuIa6f5o"
      },
      "source": [
        "#Importaciones\n",
        "import random, os, shutil\n",
        "import imageio\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygTol8R3KNrI"
      },
      "source": [
        "# **Descomprimimos la base de datos (unzip)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq6WKNncEm-W"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "cont = 0\n",
        "for cont in range(1,21): #cont = 1\n",
        "  if cont < 10 and cont != 5 and cont != 6:\n",
        "    file_name = '/content/drive/My Drive/Terravic_Facial_IR_Database/face0{}.zip'.format(cont)\n",
        "  elif cont >= 10:\n",
        "    file_name = '/content/drive/My Drive/Terravic_Facial_IR_Database/face{}.zip'.format(cont)\n",
        "  with ZipFile(file_name, 'r') as zip: \n",
        "    zip.extractall('Terravic_Original') "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-34D66Z56tV6"
      },
      "source": [
        "# **Renombramos las clases de la base de datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyujGlV2ow2a"
      },
      "source": [
        "Esta operación es importante para poder manejar con mayor facilidad los nombres de las carpetas que contienen las imágenes de cada individuo, ya que se debe considerar que las colecciones 5 y 6 están dañadas y no es posible acceder a ellas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6Gz10m7q5dj"
      },
      "source": [
        "import os\n",
        "\n",
        "for i in range(1,21):\n",
        "  if i >= 7 and i <= 9:\n",
        "    os.rename('Terravic_Original/face0{}'.format(i), 'Terravic_Original/face0{}'.format(i - 2))\n",
        "  elif i >= 10 and i <= 11:\n",
        "    os.rename('Terravic_Original/face{}'.format(i), 'Terravic_Original/face0{}'.format(i - 2))\n",
        "  elif i >= 12:\n",
        "    os.rename('Terravic_Original/face{}'.format(i), 'Terravic_Original/face{}'.format(i - 2))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvMCmqgQX3T-"
      },
      "source": [
        "# **Construcción de los conjuntos de entrenamiento, validación y prueba**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqvrp83qTucK"
      },
      "source": [
        "**Ordenamos la base de datos aleatoriamente**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N69mgHHOgw0w"
      },
      "source": [
        "os.mkdir('Terravic_Shuffled')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5MHJGiUjEUu"
      },
      "source": [
        "#Creación de carpetas\n",
        "for i in range(1,19):\n",
        "  if i < 10: #i = 0\n",
        "    dest_train = 'Terravic_Shuffled/face0{}/'.format(i) # dest_train = 'Terravic_Shuffled/face01/'\n",
        "  elif i >= 10:\n",
        "    dest_train = 'Terravic_Shuffled/face{}/'.format(i)\n",
        "  os.mkdir(dest_train)\n",
        "\n",
        "\n",
        "for index_class in range(1,19): #index_class = 1\n",
        "  sample = 0\n",
        "  if index_class < 10: \n",
        "    class_path = 'Terravic_Original/face0{}/'.format(index_class) #class_path = 'Terravic_Original/face01/'\n",
        "  elif index_class >= 10:\n",
        "    class_path = 'Terravic_Original/face{}/'.format(index_class)\n",
        "\n",
        "  #Extracción de las imágenes correspondientes a cada individuo\n",
        "  lst = sorted(os.listdir(class_path))\n",
        "\n",
        "  #Shuffling the dataset\n",
        "  random.shuffle(lst)\n",
        "\n",
        "  for file_name in lst:\n",
        "    img_person = imageio.imread(class_path + file_name)\n",
        "    if index_class < 10:\n",
        "      if sample < 10:                                                       #0000.jpg\n",
        "        imageio.imwrite('Terravic_Shuffled/face0{}/'.format(index_class) + '000{}.jpg'.format(sample), img_person)\n",
        "      elif sample >= 10 and sample < 100:\n",
        "        imageio.imwrite('Terravic_Shuffled/face0{}/'.format(index_class) + '00{}.jpg'.format(sample), img_person)\n",
        "      elif sample >= 100 and sample < 1000:\n",
        "        imageio.imwrite('Terravic_Shuffled/face0{}/'.format(index_class) + '0{}.jpg'.format(sample), img_person)\n",
        "      else:\n",
        "        imageio.imwrite('Terravic_Shuffled/face0{}/'.format(index_class) + '{}.jpg'.format(sample), img_person)\n",
        "      \n",
        "      sample = sample + 1\n",
        "\n",
        "    else:\n",
        "      if sample < 10:\n",
        "        imageio.imwrite('Terravic_Shuffled/face{}/'.format(index_class) + '000{}.jpg'.format(sample), img_person)\n",
        "      elif sample >= 10 and sample < 100:\n",
        "        imageio.imwrite('Terravic_Shuffled/face{}/'.format(index_class) + '00{}.jpg'.format(sample), img_person)\n",
        "      elif sample >= 100 and sample < 1000:\n",
        "        imageio.imwrite('Terravic_Shuffled/face{}/'.format(index_class) + '0{}.jpg'.format(sample), img_person)\n",
        "      else:\n",
        "        imageio.imwrite('Terravic_Shuffled/face{}/'.format(index_class) + '{}.jpg'.format(sample), img_person)\n",
        "      \n",
        "      sample = sample + 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDVW2l3w0UeG",
        "outputId": "5f6575fa-8dab-4fe9-9541-a88846bb6a72"
      },
      "source": [
        "#Comprobación\n",
        "import os\n",
        "for i in range(1,19):\n",
        "  if i < 10:\n",
        "    print('total  images in test_original/face0{}:'.format(i), len(os.listdir('Terravic_Shuffled/face0{}/'.format(i))))\n",
        "  else:\n",
        "    print('total  images in test_original/face{}:'.format(i), len(os.listdir('Terravic_Shuffled/face{}/'.format(i))))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total  images in test_original/face01: 227\n",
            "total  images in test_original/face02: 620\n",
            "total  images in test_original/face03: 592\n",
            "total  images in test_original/face04: 487\n",
            "total  images in test_original/face05: 1297\n",
            "total  images in test_original/face06: 857\n",
            "total  images in test_original/face07: 1117\n",
            "total  images in test_original/face08: 283\n",
            "total  images in test_original/face09: 434\n",
            "total  images in test_original/face10: 2179\n",
            "total  images in test_original/face11: 1417\n",
            "total  images in test_original/face12: 1482\n",
            "total  images in test_original/face13: 1125\n",
            "total  images in test_original/face14: 1611\n",
            "total  images in test_original/face15: 2632\n",
            "total  images in test_original/face16: 2215\n",
            "total  images in test_original/face17: 2539\n",
            "total  images in test_original/face18: 1670\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-yPvA0hWA6u"
      },
      "source": [
        "**Creación de las carpetas de entrenamiento, validación y prueba, con sus respectivas carpetas para cada individuo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZi6kEWLct8C"
      },
      "source": [
        "datasets = ['train', 'validation', 'test']\n",
        "\n",
        "for dataset_name in datasets:\n",
        "  os.mkdir(dataset_name)\n",
        "  for i in range(1,19):\n",
        "    if i < 10:\n",
        "      dest_train = dataset_name+'/face0{}/'.format(i) #dest_train = 'train/face02/\n",
        "    elif i >= 10:\n",
        "      dest_train = dataset_name+'/face{}/'.format(i)\n",
        "    os.mkdir(dest_train)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSmRefzrlhT-"
      },
      "source": [
        "def fill_dataset(src_dataset, dest_dataset, face_index, limit_low, limit_upper): \n",
        "#fill_dataset('Terravic_Shuffled','test', 1, limit_validation + 127, limit_validation + limit_test + 127) \n",
        "  if face_index < 10:\n",
        "    src_path = src_dataset+'/face0{}/'.format(face_index)\n",
        "    dest_path = dest_dataset+'/face0{}/'.format(face_index)\n",
        "  else:\n",
        "    src_path = src_dataset+'/face{}/'.format(face_index)\n",
        "    dest_path = dest_dataset+'/face{}/'.format(face_index)\n",
        "\n",
        "  lst = sorted(os.listdir(src_path))\n",
        "        \n",
        "  for file_name in lst[limit_low:limit_upper]: #lst[177,227]\n",
        "    if limit_low < limit_upper:\n",
        "      img_original = imageio.imread(src_path + file_name)\n",
        "      if limit_low < 10:\n",
        "          imageio.imwrite(dest_path + '000{}.jpg'.format(limit_low), img_original)\n",
        "      elif limit_low >= 10 and limit_low < 100:\n",
        "          imageio.imwrite(dest_path + '00{}.jpg'.format(limit_low), img_original)\n",
        "      else: \n",
        "          imageio.imwrite(dest_path + '0{}.jpg'.format(limit_low), img_original)\n",
        "              \n",
        "      limit_low = limit_low + 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GxpOXUDOCWj"
      },
      "source": [
        "**Asignación de imágenes a cada conjunto**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXEj5jLD0wqn",
        "outputId": "96284ab5-8afe-4aeb-e671-6e0e1c9a48c4"
      },
      "source": [
        "for i in range (1, 19):\n",
        "  if i < 10:\n",
        "    total = len(os.listdir('Terravic_Shuffled/face0{}'.format(i))) # person01 tiene 227, total = 227\n",
        "  else:\n",
        "    total = len(os.listdir('Terravic_Shuffled/face{}'.format(i)))\n",
        "  limit1 = total - 127 # limit1 = 1482 - 127 = 1355\n",
        "  if limit1 % 2 == 0:\n",
        "    limit_validation = int(limit1 / 2) #limit_validation = 50\n",
        "    limit_test = limit_validation ##limit_test = 50\n",
        "  else:\n",
        "    limit_validation = limit1 // 2 #-> limit_validation = 677\n",
        "    limit_test = (limit1 // 2) + 1 #-> limit_test = 678\n",
        "\n",
        "  fill_dataset('Terravic_Shuffled','train', i, 0, 127)\n",
        "  print(limit_validation + 127)\n",
        "  fill_dataset('Terravic_Shuffled','validation', i, 127, limit_validation + 127) \n",
        "  fill_dataset('Terravic_Shuffled','test', i, limit_validation + 127, limit_validation + limit_test + 127)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "177\n",
            "373\n",
            "359\n",
            "307\n",
            "712\n",
            "492\n",
            "622\n",
            "205\n",
            "280\n",
            "1153\n",
            "772\n",
            "804\n",
            "626\n",
            "869\n",
            "1379\n",
            "1171\n",
            "1333\n",
            "898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKkLnurGluJN"
      },
      "source": [
        "#Face 1 (train, validation and test sets)\n",
        "fill_dataset('Terravic_Shuffled','train', 1, 0, 127)\n",
        "fill_dataset('Terravic_Shuffled','validation', 1, 127, 177)\n",
        "fill_dataset('Terravic_Shuffled','test', 1, 177, 227)\n",
        "\n",
        "#Face 2 (train and validation sets)\n",
        "fill_dataset('Terravic_Shuffled','train', 2, 0, 127)\n",
        "fill_dataset('Terravic_Shuffled','validation', 2, 127, 374)\n",
        "fill_dataset('Terravic_Shuffled','test', 2, 374, 620)\n",
        "\n",
        "#Face 3 (train and validation sets)\n",
        "fill_dataset('Terravic_Shuffled','train', 3, 0, 127)\n",
        "fill_dataset('Terravic_Shuffled','validation', 3, 127, 360)\n",
        "fill_dataset('Terravic_Shuffled','test', 3, 360, 592)\n",
        "\n",
        "#Face 4 (train and validation sets)\n",
        "fill_dataset('Terravic_Shuffled','train', 4, 0, 127)\n",
        "fill_dataset('Terravic_Shuffled','validation', 4, 127, 307)\n",
        "fill_dataset('Terravic_Shuffled','test', 4, 307, 487)\n",
        "\n",
        "#Face 5 (train and validation sets)\n",
        "fill_dataset('Terravic_Shuffled','train', 5, 0, 127)\n",
        "fill_dataset('Terravic_Shuffled','validation', 5, 127, 712)\n",
        "fill_dataset('Terravic_Shuffled','test', 5, 712, 1297)\n",
        "\n",
        "#Face 6 (train and validation sets)\n",
        "fill_dataset('Terravic_Shuffled','train', 6, 0, 127)\n",
        "fill_dataset('Terravic_Shuffled','validation', 6, 127, 492)\n",
        "fill_dataset('Terravic_Shuffled','test', 6, 492, 857)\n",
        "\n",
        "#Face 7 (train and validation sets)\n",
        "fill_dataset('Terravic_Shuffled','train', 7, 0, 127)\n",
        "fill_dataset('Terravic_Shuffled','validation', 7, 127, 622)\n",
        "fill_dataset('Terravic_Shuffled','test', 7, 622, 1117)\n",
        "\n",
        "#Face 8 (train and validation sets)\n",
        "fill_dataset('Terravic_Shuffled','train', 8, 0, 127)\n",
        "fill_dataset('Terravic_Shuffled','validation', 8, 127, 205)\n",
        "fill_dataset('Terravic_Shuffled','test', 8, 205, 283)\n",
        "\n",
        "#Face 9 (train and validation sets)\n",
        "fill_dataset('Terravic_Shuffled','train', 9, 0, 127)\n",
        "fill_dataset('Terravic_Shuffled','validation', 9, 127, 281)\n",
        "fill_dataset('Terravic_Shuffled','test', 9, 281, 434)\n",
        "\n",
        "#Face 10 (train and validation sets)\n",
        "fill_dataset('Terravic_Shuffled','train', 10, 0, 127)\n",
        "fill_dataset('Terravic_Shuffled','validation', 10, 127, 1153)\n",
        "fill_dataset('Terravic_Shuffled','test', 10, 1153, 2179)\n",
        "\n",
        "#Face 11 (train and validation sets)\n",
        "fill_dataset('Terravic_Shuffled','train', 11, 0, 127)\n",
        "fill_dataset('Terravic_Shuffled','validation', 11, 127, 772)\n",
        "fill_dataset('Terravic_Shuffled','test', 11, 772, 1417)\n",
        "\n",
        "#Face 12 (train and validation sets)\n",
        "fill_dataset('Terravic_Shuffled','train', 12, 0, 127)\n",
        "fill_dataset('Terravic_Shuffled','validation', 12, 127, 805)\n",
        "fill_dataset('Terravic_Shuffled','test', 12, 805, 1482)\n",
        "\n",
        "#Face 13 (train and validation sets)\n",
        "fill_dataset('Terravic_Shuffled','train', 13, 0, 127)\n",
        "fill_dataset('Terravic_Shuffled','validation', 13, 127, 626)\n",
        "fill_dataset('Terravic_Shuffled','test', 13, 626, 1125)\n",
        "\n",
        "#Face 14 (train and validation sets)\n",
        "fill_dataset('Terravic_Shuffled','train', 14, 0, 127)\n",
        "fill_dataset('Terravic_Shuffled','validation', 14, 127, 869)\n",
        "fill_dataset('Terravic_Shuffled','test', 14, 869, 1611)\n",
        "\n",
        "#Face 15 (train and validation sets)\n",
        "fill_dataset('Terravic_Shuffled','train', 15, 0, 127)\n",
        "fill_dataset('Terravic_Shuffled','validation', 15, 127, 1380)\n",
        "fill_dataset('Terravic_Shuffled','test', 15, 1380, 2632)\n",
        "\n",
        "#Face 16 (train and validation sets)\n",
        "fill_dataset('Terravic_Shuffled','train', 16, 0, 127)\n",
        "fill_dataset('Terravic_Shuffled','validation', 16, 127, 1171)\n",
        "fill_dataset('Terravic_Shuffled','test', 16, 1171, 2215)\n",
        "\n",
        "#Face 17 (train and validation sets)\n",
        "fill_dataset('Terravic_Shuffled','train', 17, 0, 127)\n",
        "fill_dataset('Terravic_Shuffled','validation', 17, 127, 1333)\n",
        "fill_dataset('Terravic_Shuffled','test', 17, 1333, 2539)\n",
        "\n",
        "#Face 18 (train and validation sets)\n",
        "fill_dataset('Terravic_Shuffled','train', 18, 0, 127)\n",
        "fill_dataset('Terravic_Shuffled','validation', 18, 127, 899)\n",
        "fill_dataset('Terravic_Shuffled','test', 18, 899, 1670)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky7uBJckZIJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be38c059-97d7-49f6-8c83-624190637a08"
      },
      "source": [
        "#Comprobación\n",
        "import os\n",
        "for i in range(1, 19):\n",
        "  if i < 10:\n",
        "    print('total  images in train/face0{}:'.format(i), len(os.listdir('train/face0{}/'.format(i))))\n",
        "    print('total  images in validation/face0{}:'.format(i), len(os.listdir('validation/face0{}/'.format(i))))\n",
        "    print('total  images in test/face0{}:'.format(i), len(os.listdir('test/face0{}/'.format(i))))\n",
        "  else:\n",
        "    print('total  images in train/face{}:'.format(i), len(os.listdir('train/face{}/'.format(i))))\n",
        "    print('total  images in validation/face{}:'.format(i), len(os.listdir('validation/face{}/'.format(i))))\n",
        "    print('total  images in test/face{}:'.format(i), len(os.listdir('test/face{}/'.format(i))))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total  images in train/face01: 127\n",
            "total  images in validation/face01: 50\n",
            "total  images in test/face01: 50\n",
            "total  images in train/face02: 127\n",
            "total  images in validation/face02: 247\n",
            "total  images in test/face02: 246\n",
            "total  images in train/face03: 127\n",
            "total  images in validation/face03: 233\n",
            "total  images in test/face03: 232\n",
            "total  images in train/face04: 127\n",
            "total  images in validation/face04: 180\n",
            "total  images in test/face04: 180\n",
            "total  images in train/face05: 127\n",
            "total  images in validation/face05: 585\n",
            "total  images in test/face05: 585\n",
            "total  images in train/face06: 127\n",
            "total  images in validation/face06: 365\n",
            "total  images in test/face06: 365\n",
            "total  images in train/face07: 127\n",
            "total  images in validation/face07: 495\n",
            "total  images in test/face07: 495\n",
            "total  images in train/face08: 127\n",
            "total  images in validation/face08: 78\n",
            "total  images in test/face08: 78\n",
            "total  images in train/face09: 127\n",
            "total  images in validation/face09: 154\n",
            "total  images in test/face09: 153\n",
            "total  images in train/face10: 127\n",
            "total  images in validation/face10: 1026\n",
            "total  images in test/face10: 1026\n",
            "total  images in train/face11: 127\n",
            "total  images in validation/face11: 645\n",
            "total  images in test/face11: 645\n",
            "total  images in train/face12: 127\n",
            "total  images in validation/face12: 678\n",
            "total  images in test/face12: 677\n",
            "total  images in train/face13: 127\n",
            "total  images in validation/face13: 499\n",
            "total  images in test/face13: 499\n",
            "total  images in train/face14: 127\n",
            "total  images in validation/face14: 742\n",
            "total  images in test/face14: 742\n",
            "total  images in train/face15: 127\n",
            "total  images in validation/face15: 1253\n",
            "total  images in test/face15: 1252\n",
            "total  images in train/face16: 127\n",
            "total  images in validation/face16: 1044\n",
            "total  images in test/face16: 1044\n",
            "total  images in train/face17: 127\n",
            "total  images in validation/face17: 1206\n",
            "total  images in test/face17: 1206\n",
            "total  images in train/face18: 127\n",
            "total  images in validation/face18: 772\n",
            "total  images in test/face18: 771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb5aCKv_QZR4"
      },
      "source": [
        "# **Sistema de reconocimiento facial**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDGU7sI6JlPY"
      },
      "source": [
        "**Referenciamos los conjuntos de entrenamiento, validación y prueba**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUZHNVy_RRs1"
      },
      "source": [
        "train_dir = os.path.join('train')\n",
        "validation_dir = os.path.join('validation')\n",
        "test_dir = os.path.join('test')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7WWbw6JJp_G"
      },
      "source": [
        "**Carga de la arquitectura VGG16**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrqAdgeRQdM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee87ac3-b10a-4f6d-f035-9cead05c35a8"
      },
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "conv_base = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(72, 96, 3)) # input_shape = (largo, ancho, canales)\n",
        "\n",
        "conv_base.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 72, 96, 3)]       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 72, 96, 64)        1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 72, 96, 64)        36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 36, 48, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 36, 48, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 36, 48, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 18, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 18, 24, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 18, 24, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 18, 24, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 9, 12, 256)        0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 9, 12, 512)        1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 9, 12, 512)        2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 9, 12, 512)        2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 4, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 4, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 4, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 4, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 2, 3, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q6SaotDJvml"
      },
      "source": [
        "**Congelamos y descongelamos ciertas capas (Fine-tuning)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GToxLhHtQxSm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87aeb692-dbba-4035-8231-71d98d36a750"
      },
      "source": [
        "for layer in conv_base.layers:\n",
        "    if layer.name[:6] == 'block5': \n",
        "      layer.trainable = True\n",
        "    else:\n",
        "      layer.trainable = False\n",
        "\n",
        "conv_base.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 72, 96, 3)]       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 72, 96, 64)        1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 72, 96, 64)        36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 36, 48, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 36, 48, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 36, 48, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 18, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 18, 24, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 18, 24, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 18, 24, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 9, 12, 256)        0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 9, 12, 512)        1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 9, 12, 512)        2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 9, 12, 512)        2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 4, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 4, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 4, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 4, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 2, 3, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 7,079,424\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysYRWbXVQ_lV"
      },
      "source": [
        "# **Definición de la arquitectura (incluyendo el módulo de transfer learning)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2LGYLGqQ-x1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02687b98-cfa8-4955-9636-dc4c18559b5d"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.BatchNormalization()) #Normalizar los pesos aprendidos hasta este punto\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(18, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 2, 3, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1, 1, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 18)                9234      \n",
            "=================================================================\n",
            "Total params: 14,725,970\n",
            "Trainable params: 7,089,682\n",
            "Non-trainable params: 7,636,288\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BHBw-s_KD-N"
      },
      "source": [
        "# **Compilación**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOjtTtIBRill"
      },
      "source": [
        "from tensorflow.keras import optimizers\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc']) #'acc' -> accuracy -> precisión"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLFUQRRtKH8F"
      },
      "source": [
        "# **Definición de los generadores**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUp2jo6lRqSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ebc4753-15a6-4542-b25c-e59c1887a1a7"
      },
      "source": [
        "#Using ImageDataGenerator to read images from directories\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir, # Target directory\n",
        "        target_size=(72, 96), # All images are resized from 240x320 to 72x96\n",
        "        batch_size= 9, \n",
        "        color_mode='rgb',\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(72, 96),\n",
        "        batch_size=1,\n",
        "        color_mode='rgb',\n",
        "        class_mode='categorical')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2286 images belonging to 18 classes.\n",
            "Found 10252 images belonging to 18 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-bRqMJZKLS9"
      },
      "source": [
        "# **Entrenamiento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIbSj6NJR35o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "697ed828-6018-47ab-b4c8-d0a514583cf4"
      },
      "source": [
        "#Training and validation stages\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=254, #70\n",
        "      epochs= 10,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=10252)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "254/254 [==============================] - 58s 229ms/step - loss: 0.0776 - acc: 0.9847 - val_loss: 4.2827e-04 - val_acc: 1.0000\n",
            "Epoch 2/10\n",
            "254/254 [==============================] - 58s 228ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
            "Epoch 3/10\n",
            "254/254 [==============================] - 58s 227ms/step - loss: 5.1276e-05 - acc: 1.0000 - val_loss: 5.8622e-06 - val_acc: 1.0000\n",
            "Epoch 4/10\n",
            "254/254 [==============================] - 58s 227ms/step - loss: 1.6254e-04 - acc: 1.0000 - val_loss: 2.5424e-06 - val_acc: 1.0000\n",
            "Epoch 5/10\n",
            "252/254 [============================>.] - ETA: 0s - loss: 5.1764e-05 - acc: 1.0000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-9e26e98044ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       validation_steps=10252)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m   1134\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TraceContext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4AIfeaS8xcN"
      },
      "source": [
        "# **Curvas de precisión y pérdida**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOyTdAbv8xcP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "ec9efe08-8959-4d10-c4a2-d3be1328a56c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "max_val_acc = max(val_acc)\n",
        "max_val_acc_epoch= val_acc.index(max(val_acc)) + 1\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "min_val_loss = min(val_loss)\n",
        "min_val_loss_epoch= val_loss.index(min(val_loss)) + 1\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'go', label='Precisión en el entrenamiento')\n",
        "plt.plot(epochs, val_acc, 'r', label='Precisión en la validación')\n",
        "plt.plot(max_val_acc_epoch, max_val_acc, 'bo', label='Maximum accuracy')\n",
        "plt.title('Precisión durante el entrenamiento y la validación')\n",
        "plt.legend()\n",
        "print(\"Best accuracy epoch : % d, Value : % .1f\" %(max_val_acc_epoch, max_val_acc)) \n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'go', label='Pérdida en el entrenamiento')\n",
        "plt.plot(epochs, val_loss, 'r', label='Pérdida en la validación')\n",
        "plt.plot(min_val_loss_epoch, min_val_loss, 'bo', label='Minimum loss')\n",
        "plt.title('Pérdida durante el entrenamiento y la validación')\n",
        "plt.legend()\n",
        "print(\"Best loss epoch : % d, Value : % .10f\" %(min_val_loss_epoch, min_val_loss))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-58243a6e143d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmax_val_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zd-LQyEeHzV"
      },
      "source": [
        "# **Re-entrenamiento del modelo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPaLLNt0eT1y"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(18, activation='softmax'))\n",
        "\n",
        "from keras import optimizers\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NxtTV9Lfa8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8f8f8b7-6e37-40e4-9a8d-6f33a87ceb58"
      },
      "source": [
        "#Training and validation stages\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=254, #70\n",
        "      epochs= 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "254/254 [==============================] - 5s 21ms/step - loss: 0.0568 - acc: 0.9891\n",
            "Epoch 2/3\n",
            "254/254 [==============================] - 5s 21ms/step - loss: 4.2741e-05 - acc: 1.0000\n",
            "Epoch 3/3\n",
            "254/254 [==============================] - 5s 21ms/step - loss: 1.8280e-05 - acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VQF2x6pzLh2"
      },
      "source": [
        "#Guardamos el modelo\n",
        "model.save('/content/drive/My Drive/Terravic_Facial_IR_Database/face_recognition_model1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lHMJ-n2zXGT"
      },
      "source": [
        "#Cargamos el modelo\n",
        "import tensorflow as tf\n",
        "model = tf.keras.models.load_model('/content/drive/My Drive/Terravic_Facial_IR_Database/face_recognition_model1.h5', compile=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgeW-BUuKQwl"
      },
      "source": [
        "# **Evaluación del modelo final**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl4H8Zy5edcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8043c023-9492-4d82-a525-65b68198a41d"
      },
      "source": [
        "#Test stage\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(72, 96),\n",
        "        batch_size=1,\n",
        "        color_mode='rgb',\n",
        "        class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10246 images belonging to 18 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLMb3p4FdBcM",
        "outputId": "7b1867dc-fdf2-4938-c8c4-a2a8857b92b5"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_generator, steps=10252)\n",
        "print('Recognition rate: ', test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10237/10252 [============================>.] - ETA: 0s - loss: 4.9650e-07 - acc: 1.0000WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10252 batches). You may need to use the repeat() function when building your dataset.\n",
            "10246/10252 [============================>.] - 53s 5ms/step - loss: 4.9609e-07 - acc: 1.0000\n",
            "Recognition rate:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOdpKOiedg83",
        "outputId": "e1c6d83a-17cb-4625-ec99-83f58d1fb600"
      },
      "source": [
        "print('Recognition rate: ', test_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recognition rate:  4.960852493240964e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B741ANM4J1d"
      },
      "source": [
        "# **Predicciones**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3e_8d9L2mYD"
      },
      "source": [
        "**Particular**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1VfdtbNncQr"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "width = 96\n",
        "height = 72\n",
        "image_face = Image.open('test/face17/01349.jpg')\n",
        "image_face = image_face.resize((width, height), Image.ANTIALIAS)\n",
        "image_face = np.array(image_face) #La imagen se convierte en un arreglo\n",
        "image_face = image_face / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJHLuNQSgTo2",
        "outputId": "6fcb87a4-772b-4dd5-9934-f6fcd7fe61b6"
      },
      "source": [
        "image_face = np.expand_dims(image_face, axis = 0)\n",
        "image_face = np.expand_dims(image_face, axis = -1) # image_face: (1, 72, 96, 1) #Tensor\n",
        "image_face = np.stack((image_face[:,:,:,0], image_face[:,:,:,0], image_face[:,:,:,0]), axis=3) # image_face: (1, 72, 96, 3)\n",
        "print(image_face.shape)\n",
        "prediction = model.predict(image_face)\n",
        "print('La imagen pertenece a la persona', np.argmax(prediction)+1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 72, 96, 3)\n",
            "La imagen pertenece a la persona 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-xRUS-v2oKr"
      },
      "source": [
        "**General**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CUpXKwO1WY7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "b8ca3261-480f-4db2-fe51-d72bdaa8b42e"
      },
      "source": [
        "predictions = []\n",
        "labels = []\n",
        "for individual in range(1,19):\n",
        "  if (individual) < 10:\n",
        "    person = 'face0{}/'.format(individual)\n",
        "  elif (individual) >= 10:\n",
        "    person = 'face{}/'.format(individual)\n",
        "  for index in range(len(os.listdir('train/'+person)) + len(os.listdir('validation/'+person)), len(os.listdir('train/'+person)) + len(os.listdir('validation/'+person)) + len(os.listdir('test/'+person))):\n",
        "    width = 96\n",
        "    height = 72\n",
        "    if index < 100:\n",
        "      image_face = Image.open('test/'+person+'00{}.jpg'.format(index))\n",
        "    else:\n",
        "      image_face = Image.open('test/'+person+'0{}.jpg'.format(index))\n",
        "    image_face = image_face.resize((width, height), Image.ANTIALIAS)\n",
        "    image_face = np.array(image_face)\n",
        "    image_face = image_face / 255.0\n",
        "    image_face = np.expand_dims(image_face, axis = 0)\n",
        "    image_face = np.expand_dims(image_face, axis = -1)\n",
        "    image_face = np.stack((image_face[:,:,:,0], image_face[:,:,:,0], image_face[:,:,:,0]), axis=3)\n",
        "    prediction = model.predict(image_face)\n",
        "    predictions.append(np.argmax(prediction)+1)\n",
        "    labels.append(individual)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-0f5f67b98d97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mimage_face\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_face\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mimage_face\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_face\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_face\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_face\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_face\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindividual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1593\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec, job_token)\u001b[0m\n\u001b[1;32m    694\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    720\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m    721\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_job_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         gen_experimental_dataset_ops.make_data_service_iterator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3005\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   3006\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MakeIterator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3007\u001b[0;31m         tld.op_callbacks, dataset, iterator)\n\u001b[0m\u001b[1;32m   3008\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3009\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mul7jpwtu5bf"
      },
      "source": [
        "predictions = []\n",
        "labels = []\n",
        "for individual in range(1,19):\n",
        "  if (individual) < 10:\n",
        "    face = 'face0{}/'.format(individual)\n",
        "  elif (individual) >= 10:\n",
        "    face = 'face{}/'.format(individual)\n",
        "  for index in range(len(os.listdir('train/'+face)) + len(os.listdir('validation/'+face)), len(os.listdir('train/'+face)) + len(os.listdir('validation/'+face)) + len(os.listdir('test/'+face))):\n",
        "    width = 96\n",
        "    height = 72\n",
        "    if index < 100:\n",
        "      image_face = Image.open('test/'+face+'00{}.jpg'.format(index))\n",
        "    else:\n",
        "      image_face = Image.open('test/'+face+'0{}.jpg'.format(index))\n",
        "    image_face = image_face.resize((width, height), Image.ANTIALIAS)\n",
        "    image_face = np.array(image_face)\n",
        "    image_face = image_face / 255.0\n",
        "    image_face = np.expand_dims(image_face, axis = 0)\n",
        "    image_face = np.expand_dims(image_face, axis = -1)\n",
        "    image_face = np.stack((image_face[:,:,:,0], image_face[:,:,:,0], image_face[:,:,:,0]), axis=3)\n",
        "    prediction = model.predict(image_face)\n",
        "    predictions.append(np.argmax(prediction)+1)\n",
        "    labels.append(individual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhcsxDg8_m3I"
      },
      "source": [
        "# **Evaluación del desempeño mediante matriz de confusión y F1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wcdn1KOH_YDn"
      },
      "source": [
        "**Matriz de confusión**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRRuGE8l_aCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba4c97b-80e1-44ac-f061-117b4a8e0db2"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "mcm = confusion_matrix(labels, predictions)\n",
        "print(mcm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  50    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0]\n",
            " [   0  247    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0]\n",
            " [   0    0  233    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0]\n",
            " [   0    0    0  180    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0]\n",
            " [   0    0    0    0  585    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0]\n",
            " [   0    0    0    0    0  365    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0]\n",
            " [   0    0    0    0    0    0  495    0    0    0    0    0    0    0\n",
            "     0    0    0    0]\n",
            " [   0    0    0    0    0    0    0   78    0    0    0    0    0    0\n",
            "     0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0  154    0    0    0    0    0\n",
            "     0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0 1026    0    0    0    0\n",
            "     0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0  645    0    0    0\n",
            "     0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0  678    0    0\n",
            "     0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0  499    0\n",
            "     0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0  742\n",
            "     0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "  1253    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0 1044    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0 1206    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0  772]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfvhAlBdms5K"
      },
      "source": [
        "**Individuo 1 vs Todos los demás** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVtEkxKBnE-O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "45bca120-617f-416b-9b86-a9ad0076f4b1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.matshow(mcm, cmap=plt.cm.Blues, alpha=0.5)\n",
        "for i in range(mcm.shape[0]):\n",
        "  for j in range(mcm.shape[1]):\n",
        "      ax.text(x=j, y=i, s=mcm[i, j], va='center', ha='center')\n",
        "\n",
        "plt.title(\"Matriz de confusión\",fontsize=15)\n",
        "#plt.title('f model: T= %d' % (t))\n",
        "plt.xlabel('Predicciones')\n",
        "plt.ylabel('Valores verdaderos o etiquetas')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M89n5NZ6_Mi2"
      },
      "source": [
        "**Calculamos la métrica F1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P48jAGh9iD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a7de3fa-645e-4b3a-9f57-3ca68d1f907d"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score, f1_score\n",
        "\n",
        "print('Precision: %.7f' % precision_score(y_true=labels, y_pred=predictions, average='macro'))\n",
        "print('Recall: %.7f' % recall_score(y_true=labels, y_pred=predictions, average='macro'))\n",
        "print('F1: %.7f' % f1_score(y_true=labels, y_pred=predictions, average='macro'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 1.0000000\n",
            "Recall: 1.0000000\n",
            "F1: 1.0000000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}